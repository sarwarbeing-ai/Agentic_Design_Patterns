{"cells":[{"cell_type":"code","source":["from langchain.memory import ChatMessageHistory\n","\n","# Initialize the history object\n","history = ChatMessageHistory()\n","\n","# Add user and AI messages\n","history.add_user_message(\"I'm heading to New York next week.\")\n","history.add_ai_message(\"Great! It's a fantastic city.\")\n","\n","# Access the list of messages\n","print(history.messages)"],"outputs":[],"execution_count":null,"metadata":{"id":"2EzLzlDUViAT"}},{"cell_type":"code","source":["from langchain.memory import ConversationBufferMemory\n","\n","# Initialize memory\n","memory = ConversationBufferMemory()\n","\n","# Save a conversation turn\n","memory.save_context({\"input\": \"What's the weather like?\"}, {\"output\": \"It's sunny today.\"})\n","\n","# Load the memory as a string\n","print(memory.load_memory_variables({}))"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'history': \"Human: What's the weather like?\\nAI: It's sunny today.\"}\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1-1418393889.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory()\n"]}],"execution_count":null,"metadata":{"id":"PsBeTnUoViAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751345929854,"user_tz":-120,"elapsed":2897,"user":{"displayName":"Antonio Gulli","userId":"17769953396342459304"}},"outputId":"d17b123c-577a-43cd-e437-8d4795436ac0"}},{"cell_type":"code","source":["from langchain_openai import OpenAI\n","from langchain.chains import LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","\n","# 1. Define LLM and Prompt\n","llm = OpenAI(temperature=0)\n","template = \"\"\"You are a helpful travel agent.\n","\n","Previous conversation:\n","{history}\n","\n","New question: {question}\n","Response:\"\"\"\n","prompt = PromptTemplate.from_template(template)\n","\n","# 2. Configure Memory\n","# The memory_key \"history\" matches the variable in the prompt\n","memory = ConversationBufferMemory(memory_key=\"history\")\n","\n","# 3. Build the Chain\n","conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n","\n","# 4. Run the Conversation\n","response = conversation.predict(question=\"I want to book a flight.\")\n","print(response)\n","response = conversation.predict(question=\"My name is Sam, by the way.\")\n","print(response)\n","response = conversation.predict(question=\"What was my name again?\")\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"qorPJSjAWfEQ","executionInfo":{"status":"error","timestamp":1751345955730,"user_tz":-120,"elapsed":88,"user":{"displayName":"Antonio Gulli","userId":"17769953396342459304"}},"outputId":"c068f7c2-6c30-494b-a386-64c13f2f66d3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'langchain_openai'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-2809770307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.chains import LLMChain\n","from langchain.memory import ConversationBufferMemory\n","from langchain_core.prompts import (\n","    ChatPromptTemplate,\n","    MessagesPlaceholder,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","\n","# 1. Define Chat Model and Prompt\n","llm = ChatOpenAI()\n","prompt = ChatPromptTemplate(\n","    messages=[\n","        SystemMessagePromptTemplate.from_template(\"You are a friendly assistant.\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        HumanMessagePromptTemplate.from_template(\"{question}\")\n","    ]\n",")\n","\n","# 2. Configure Memory\n","# return_messages=True is essential for chat models\n","memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","\n","# 3. Build the Chain\n","conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n","\n","# 4. Run the Conversation\n","response = conversation.predict(question=\"Hi, I'm Jane.\")\n","print(response)\n","response = conversation.predict(question=\"Do you remember my name?\")\n","print(response)"],"metadata":{"id":"a_xPt-bvWoGf","executionInfo":{"status":"error","timestamp":1751347418324,"user_tz":-120,"elapsed":39,"user":{"displayName":"Antonio Gulli","userId":"17769953396342459304"}},"outputId":"024118f4-e6bf-4b13-b3c8-4bcadc505145","colab":{"base_uri":"https://localhost:8080/","height":393}},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'langchain_openai'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3-667661079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from langchain_core.prompts import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# Node that updates the agent's instructions\n","def update_instructions(state: State, store: BaseStore):\n","    namespace = (\"instructions\",)\n","    # Get the current instructions from the store\n","    current_instructions = store.search(namespace)[0]\n","\n","    # Create a prompt to ask the LLM to reflect on the conversation\n","    # and generate new, improved instructions\n","    prompt = prompt_template.format(\n","        instructions=current_instructions.value[\"instructions\"],\n","        conversation=state[\"messages\"]\n","    )\n","\n","    # Get the new instructions from the LLM\n","    output = llm.invoke(prompt)\n","    new_instructions = output['new_instructions']\n","\n","    # Save the updated instructions back to the store\n","    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n","\n","\n","# Node that uses the instructions to generate a response\n","def call_model(state: State, store: BaseStore):\n","    namespace = (\"agent_instructions\", )\n","    # Retrieve the latest instructions from the store\n","    instructions = store.get(namespace, key=\"agent_a\")[0]\n","\n","    # Use the retrieved instructions to format the prompt\n","    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n","    # ... application logic continues"],"metadata":{"id":"uT31zxNbcNPi","executionInfo":{"status":"error","timestamp":1751347530751,"user_tz":-120,"elapsed":49,"user":{"displayName":"Antonio Gulli","userId":"17769953396342459304"}},"outputId":"fe11a3c8-4d97-4adc-e20b-770cea2111c0","colab":{"base_uri":"https://localhost:8080/","height":216}},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'State' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4-3607473729.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Node that updates the agent's instructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseStore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"instructions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get the current instructions from the store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurrent_instructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'State' is not defined"]}]},{"cell_type":"code","source":["from langgraph.store.memory import InMemoryStore\n","\n","# A placeholder for a real embedding function\n","def embed(texts: list[str]) -> list[list[float]]:\n","    # In a real application, use a proper embedding model\n","    return [[1.0, 2.0] for _ in texts]\n","\n","# Initialize an in-memory store. For production, use a database-backed store.\n","store = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\n","\n","# Define a namespace for a specific user and application context\n","user_id = \"my-user\"\n","application_context = \"chitchat\"\n","namespace = (user_id, application_context)\n","\n","# 1. Put a memory into the store\n","store.put(\n","    namespace,\n","    \"a-memory\",  # The key for this memory\n","    {\n","        \"rules\": [\n","            \"User likes short, direct language\",\n","            \"User only speaks English & python\",\n","        ],\n","        \"my-key\": \"my-value\",\n","    },\n",")\n","\n","# 2. Get the memory by its namespace and key\n","item = store.get(namespace, \"a-memory\")\n","print(\"Retrieved Item:\", item)\n","\n","# 3. Search for memories within the namespace, filtering by content\n","# and sorting by vector similarity to the query.\n","items = store.search(\n","    namespace,\n","    filter={\"my-key\": \"my-value\"},\n","    query=\"language preferences\"\n",")\n","print(\"Search Results:\", items)"],"metadata":{"id":"ZMFeT6pEcor7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}