{"cells":[{"cell_type":"code","source":["import os\n","import asyncio\n","from typing import Optional\n","\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import Runnable, RunnableParallel, RunnablePassthrough\n","\n","# --- Configuration ---\n","# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n","try:\n","    llm: Optional[ChatOpenAI] = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n","    if llm:\n","        print(f\"Language model initialized: {llm.model_name}\")\n","except Exception as e:\n","    print(f\"Error initializing language model: {e}\")\n","    llm = None\n","\n","\n","# --- Define Independent Chains ---\n","# These three chains represent distinct tasks that can be executed in parallel.\n","\n","summarize_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Summarize the following topic concisely:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","questions_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Generate three interesting questions about the following topic:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","terms_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Identify 5-10 key terms from the following topic, separated by commas:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","\n","# --- Build the Parallel + Synthesis Chain ---\n","\n","# 1. Define the block of tasks to run in parallel. The results of these,\n","#    along with the original topic, will be fed into the next step.\n","map_chain = RunnableParallel(\n","    {\n","        \"summary\": summarize_chain,\n","        \"questions\": questions_chain,\n","        \"key_terms\": terms_chain,\n","        \"topic\": RunnablePassthrough(),  # Pass the original topic through\n","    }\n",")\n","\n","# 2. Define the final synthesis prompt which will combine the parallel results.\n","synthesis_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"\"\"Based on the following information:\n","     Summary: {summary}\n","     Related Questions: {questions}\n","     Key Terms: {key_terms}\n","     Synthesize a comprehensive answer.\"\"\"),\n","    (\"user\", \"Original topic: {topic}\")\n","])\n","\n","# 3. Construct the full chain by piping the parallel results directly\n","#    into the synthesis prompt, followed by the LLM and output parser.\n","full_parallel_chain = map_chain | synthesis_prompt | llm | StrOutputParser()\n","\n","\n","# --- Run the Chain ---\n","async def run_parallel_example(topic: str) -> None:\n","    \"\"\"\n","    Asynchronously invokes the parallel processing chain with a specific topic\n","    and prints the synthesized result.\n","\n","    Args:\n","        topic: The input topic to be processed by the LangChain chains.\n","    \"\"\"\n","    if not llm:\n","        print(\"LLM not initialized. Cannot run example.\")\n","        return\n","\n","    print(f\"\\n--- Running Parallel LangChain Example for Topic: '{topic}' ---\")\n","    try:\n","        # The input to `ainvoke` is the single 'topic' string, which is\n","        # then passed to each runnable in the `map_chain`.\n","        response = await full_parallel_chain.ainvoke(topic)\n","        print(\"\\n--- Final Response ---\")\n","        print(response)\n","    except Exception as e:\n","        print(f\"\\nAn error occurred during chain execution: {e}\")\n","\n","if __name__ == \"__main__\":\n","    test_topic = \"The history of space exploration\"\n","    # In Python 3.7+, asyncio.run is the standard way to run an async function.\n","    asyncio.run(run_parallel_example(test_topic))"],"outputs":[],"execution_count":null,"metadata":{"id":"dvey-vwB7RvK"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}