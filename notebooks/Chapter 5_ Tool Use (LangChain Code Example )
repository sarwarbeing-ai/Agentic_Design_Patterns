{"cells":[{"cell_type":"code","source":["import os\n","import asyncio\n","from typing import List\n","\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.tools import tool\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","\n","# --- Configuration ---\n","# Ensure your GOOGLE_API_KEY environment variable is set.\n","try:\n","    # A model with function/tool calling capabilities is required.\n","    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n","    print(f\"✅ Language model initialized: {llm.model_name}\")\n","except Exception as e:\n","    print(f\"🛑 Error initializing language model: {e}\")\n","    llm = None\n","\n","\n","# --- Define a Tool ---\n","@tool\n","def search_information(query: str) -> str:\n","    \"\"\"\n","    Provides factual information on a given topic. Use this tool to find answers to questions\n","    like 'What is the capital of France?' or 'What is the weather in London?'.\n","    \"\"\"\n","    print(f\"\\n--- 🛠️ Tool Called: search_information with query: '{query}' ---\")\n","    # Simulate a search tool with a dictionary of predefined results.\n","    simulated_results = {\n","        \"weather in london\": \"The weather in London is currently cloudy with a temperature of 15°C.\",\n","        \"capital of france\": \"The capital of France is Paris.\",\n","        \"population of earth\": \"The estimated population of Earth is around 8 billion people.\",\n","        \"tallest mountain\": \"Mount Everest is the tallest mountain above sea level.\",\n","        \"default\": f\"Simulated search result for '{query}': No specific information found, but the topic seems interesting.\"\n","    }\n","    result = simulated_results.get(query.lower(), simulated_results[\"default\"])\n","    print(f\"--- TOOL RESULT: {result} ---\")\n","    return result\n","\n","tools = [search_information]\n","\n","\n","# --- Create a Tool-Calling Agent ---\n","if llm:\n","    # This prompt template requires an `agent_scratchpad` placeholder for the agent's internal steps.\n","    agent_prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"You are a helpful assistant.\"),\n","        (\"human\", \"{input}\"),\n","        (\"placeholder\", \"{agent_scratchpad}\"),\n","    ])\n","\n","    # Create the agent, binding the LLM, tools, and prompt together.\n","    agent = create_tool_calling_agent(llm, tools, agent_prompt)\n","\n","    # AgentExecutor is the runtime that invokes the agent and executes the chosen tools.\n","    # The 'tools' argument is not needed here as they are already bound to the agent.\n","    agent_executor = AgentExecutor(agent=agent, verbose=True)\n","\n","\n","    async def run_agent_with_tool(query: str):\n","        \"\"\"Invokes the agent executor with a query and prints the final response.\"\"\"\n","        print(f\"\\n--- 🏃 Running Agent with Query: '{query}' ---\")\n","        try:\n","            response = await agent_executor.ainvoke({\"input\": query})\n","            print(\"\\n--- ✅ Final Agent Response ---\")\n","            print(response[\"output\"])\n","        except Exception as e:\n","            print(f\"\\n🛑 An error occurred during agent execution: {e}\")\n","\n","    async def main():\n","        \"\"\"Runs all agent queries concurrently.\"\"\"\n","        tasks = [\n","            run_agent_with_tool(\"What is the capital of France?\"),\n","            run_agent_with_tool(\"What's the weather like in London?\"),\n","            run_agent_with_tool(\"Tell me something about dogs.\") # Should trigger the default tool response\n","        ]\n","        await asyncio.gather(*tasks)\n","\n","    if __name__ == \"__main__\":\n","        # Run all async tasks in a single event loop.\n","        asyncio.run(main())\n","\n","else:\n","    print(\"\\nSkipping agent execution due to LLM initialization failure.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"1R7Oj2fTbMgb"}},{"cell_type":"code","source":["!pip install -q -U \"dotenv==0.9.9\" \"langchain-google-genai==2.1.8\" \"crewai==0.150.0\" \"google-adk==1.8.0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hS3Ng0KjjOJT","executionInfo":{"status":"ok","timestamp":1753782069305,"user_tz":-120,"elapsed":56429,"user":{"displayName":"Antonio Gulli","userId":"17769953396342459304"}},"outputId":"4aedf7ad-6142-4382-b86c-909f4c57c1bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n","google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os, getpass\n","import asyncio\n","import nest_asyncio\n","from typing import List\n","from dotenv import load_dotenv\n","import logging\n","\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.tools import tool as langchain_tool\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","\n","# UNCOMMENT\n","# Prompt the user securely and set API keys as an environment variables\n","os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n","\n","try:\n","   # A model with function/tool calling capabilities is required.\n","   llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n","   print(f\"✅ Language model initialized: {llm.model}\")\n","except Exception as e:\n","   print(f\"🛑 Error initializing language model: {e}\")\n","   llm = None\n","\n","# --- Define a Tool ---\n","@langchain_tool\n","def search_information(query: str) -> str:\n","   \"\"\"\n","   Provides factual information on a given topic. Use this tool to find answers to phrases\n","   like 'capital of France' or 'weather in London?'.\n","   \"\"\"\n","   print(f\"\\n--- 🛠️ Tool Called: search_information with query: '{query}' ---\")\n","   # Simulate a search tool with a dictionary of predefined results.\n","   simulated_results = {\n","       \"weather in london\": \"The weather in London is currently cloudy with a temperature of 15°C.\",\n","       \"capital of france\": \"The capital of France is Paris.\",\n","       \"population of earth\": \"The estimated population of Earth is around 8 billion people.\",\n","       \"tallest mountain\": \"Mount Everest is the tallest mountain above sea level.\",\n","       \"default\": f\"Simulated search result for '{query}': No specific information found, but the topic seems interesting.\"\n","   }\n","   result = simulated_results.get(query.lower(), simulated_results[\"default\"])\n","   print(f\"--- TOOL RESULT: {result} ---\")\n","   return result\n","\n","tools = [search_information]\n","\n","# --- Create a Tool-Calling Agent ---\n","if llm:\n","   # This prompt template requires an `agent_scratchpad` placeholder for the agent's internal steps.\n","   agent_prompt = ChatPromptTemplate.from_messages([\n","       (\"system\", \"You are a helpful assistant.\"),\n","       (\"human\", \"{input}\"),\n","       (\"placeholder\", \"{agent_scratchpad}\"),\n","   ])\n","\n","   # Create the agent, binding the LLM, tools, and prompt together.\n","   agent = create_tool_calling_agent(llm, tools, agent_prompt)\n","\n","   # AgentExecutor is the runtime that invokes the agent and executes the chosen tools.\n","   # The 'tools' argument is not needed here as they are already bound to the agent.\n","   agent_executor = AgentExecutor(agent=agent, verbose=True, tools=tools)\n","\n","async def run_agent_with_tool(query: str):\n","   \"\"\"Invokes the agent executor with a query and prints the final response.\"\"\"\n","   print(f\"\\n--- 🏃 Running Agent with Query: '{query}' ---\")\n","   try:\n","       response = await agent_executor.ainvoke({\"input\": query})\n","       print(\"\\n--- ✅ Final Agent Response ---\")\n","       print(response[\"output\"])\n","   except Exception as e:\n","       print(f\"\\n🛑 An error occurred during agent execution: {e}\")\n","\n","async def main():\n","   \"\"\"Runs all agent queries concurrently.\"\"\"\n","   tasks = [\n","       run_agent_with_tool(\"What is the capital of France?\"),\n","       run_agent_with_tool(\"What's the weather like in London?\"),\n","       run_agent_with_tool(\"Tell me something about dogs.\") # Should trigger the default tool response\n","   ]\n","   await asyncio.gather(*tasks)\n","\n","nest_asyncio.apply()\n","asyncio.run(main())"],"metadata":{"id":"FW3Eh5_OjUea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3jt8yjp7joQ2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}